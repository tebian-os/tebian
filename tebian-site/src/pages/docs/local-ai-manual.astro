---
import Layout from '../../layouts/Layout.astro';
---

<Layout title="The Local AI Manual ‚Äî Tebian">
	<main class="resource">
		<header>
			<span class="category">Resources</span>
			<h1>The Local AI Manual</h1>
			<p class="subtitle">Your Private Brain: Ollama and Local LLM Setup.</p>
		</header>

		<article class="content">
			<section class="overview">
				<h2>The Strategy</h2>
				<p>Most AI tutorials focus on API keys and monthly subscriptions. Tebian's "AI Mode" focuses on your <strong>GPU</strong>. This guide explains how to run Large Language Models (LLMs) like Llama 3 and Mistral locally on your hardware using <strong>Ollama</strong>. No cloud. No telemetry. No censorship.</p>
				
				<p>We use a C-based runner that talks directly to your CUDA (NVIDIA) or ROCm (AMD) cores, ensuring maximum performance for your private brain.</p>
			</section>

			<div class="resource-grid">
				<div class="resource-box">
					<h3>1. The Ollama Engine</h3>
					<p>Ollama is a lightweight, C-based runner for LLMs. It handles the quantization and memory management for your models, allowing them to fit into your VRAM.</p>
					<ul>
						<li><strong>Hardware Acceleration:</strong> Auto-detects NVIDIA/AMD GPUs for 100% speed.</li>
						<li><strong>Quantization:</strong> Reduces model size (e.g., 8GB to 4GB) with 99% accuracy.</li>
						<li><strong>REST API:</strong> Allows other apps (like our <code>t-ask</code>) to talk to the model.</li>
					</ul>
				</div>

				<div class="resource-box">
					<h3>2. The `t-ask` CLI Assistant</h3>
					<p>Tebian includes <code>t-ask</code>, a Go-based (and soon Rust-based) CLI tool that connects your terminal to your local AI. You can summarize files, write code, and answer questions without leaving your shell.</p>
					<ul>
						<li><strong>Piping Support:</strong> <code>cat logs.txt | t-ask "Find the error"</code>.</li>
						<li><strong>System Prompts:</strong> Pre-configured "Developer," "Writer," and "Admin" personas.</li>
						<li><strong>Context Aware:</strong> Remembers your previous questions for a seamless chat experience.</li>
					</ul>
				</div>

				<div class="resource-box">
					<h3>3. Choosing Your Model</h3>
					<p>Tebian's "AI Menu" provides one-click downloads for the world's most capable local models.</p>
					<ul>
						<li><strong>Llama 3 (Meta):</strong> The current king of open-weight models. Great for general tasks.</li>
						<li><strong>Mistral:</strong> Highly efficient and fast. Perfect for mobile or low-power machines.</li>
						<li><strong>CodeLlama:</strong> Specialized for programming and debugging.</li>
					</ul>
				</div>

				<div class="resource-box warning">
					<h3>4. Memory Management (VRAM)</h3>
					<p>Local AI is memory-intensive. To get the best speed, your model should fit entirely into your GPU's **VRAM**. Tebian's setup script helps you pick the right model size for your hardware.</p>
					<ul>
						<li><strong>4GB VRAM:</strong> Use 3B or smaller models.</li>
						<li><strong>8GB VRAM:</strong> Use 7B or 8B models (Llama 3).</li>
						<li><strong>12GB+ VRAM:</strong> Use 13B models or run multiple small models at once.</li>
					</ul>
				</div>
			</div>

			<section class="deep-dive">
				<h2>Why Local AI on Tebian?</h2>
				<p>By running AI as a system utility on a stable Debian base, you turn your computer into a true <strong>Intelligent Workstation</strong>. You aren't just a user of AI; you are the host. It's faster, more private, and completely free. One ISO. One menu. One Private Brain.</p>
			</section>
		</article>

		<div class="nav">
			<a href="/rigs">‚Üê Back to Rigs</a>
			<a href="/blog/local-intelligence">The Local AI Blog ‚Üí</a>
		</div>
	</main>
</Layout>

<style>
	.resource {
		max-width: 1000px;
		width: 100%;
		padding: 4rem 1rem;
	}

	header {
		margin-bottom: 4rem;
		text-align: center;
	}

	.category {
		color: var(--accent);
		text-transform: uppercase;
		letter-spacing: 0.1em;
		font-weight: 700;
		font-size: 0.8rem;
	}

	h1 {
		font-size: 3.5rem;
		color: #fff;
		margin: 0.5rem 0;
		line-height: 1.1;
		font-weight: 800;
	}

	.subtitle {
		color: #9399b2;
		font-size: 1.25rem;
		font-style: italic;
	}

	h2 {
		color: var(--accent);
		font-size: 2rem;
		margin: 3rem 0 1.5rem 0;
	}

	p {
		color: #9399b2;
		line-height: 1.8;
		margin-bottom: 2rem;
		font-size: 1.1rem;
	}

	.resource-grid {
		display: grid;
		grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
		gap: 2rem;
		margin-bottom: 4rem;
	}

	.resource-box {
		background: rgba(255, 255, 255, 0.02);
		padding: 2.5rem;
		border-radius: 20px;
		border: 1px solid rgba(255, 255, 255, 0.05);
	}

	.resource-box h3 {
		font-size: 1.5rem;
		color: #cdd6f4;
		margin-bottom: 1rem;
	}

	.resource-box ul {
		list-style: none;
		padding: 0;
		margin-top: 1.5rem;
		display: flex;
		flex-direction: column;
		gap: 0.75rem;
	}

	.resource-box li {
		color: #9399b2;
		padding-left: 1.5rem;
		position: relative;
		font-size: 0.95rem;
	}

	.resource-box li::before {
		content: "üß†";
		position: absolute;
		left: 0;
	}

	.nav {
		margin-top: 4rem;
		padding-top: 2rem;
		border-top: 1px solid rgba(255, 255, 255, 0.05);
		display: flex;
		justify-content: space-between;
	}

	.nav a {
		color: #6c7086;
		text-decoration: none;
		font-size: 0.9rem;
		transition: color 0.2s;
	}

	.nav a:hover {
		color: var(--accent);
	}

	@media (max-width: 640px) {
		.resource-grid { grid-template-columns: 1fr; }
	}
</style>
